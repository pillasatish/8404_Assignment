---
title: "Assignmeant_8404"
author: "SATISH"
date: "11/4/2021"
output: html_document
---
1  Cluster Analysis
#Reading the dataset
```{r}
filename="C:/Users/pilla/Desktop/8404_Assignment/Ch10Ex11.csv"
gexpr=read.csv(filename,header = FALSE)
df=gexpr
dim(df)
```
#Transpose the matrix 
```{r}
g=t(gexpr)
dim(g)
```
#(a)Hierarchical clustering(correlation-based distance)
```{r}
dim(df)
r_obs=cor(df)
d_cor=1-r_obs
d1=as.dist(d_cor)
```

```{r}
hc_c=hclust(d1,method = "single")
#class(hc_c)
#str(hc_c)
```
```{r}
plot(hc_c,cex=0.5,mian="")
```

#complete-linkage
```{r}
hc_c2=hclust(d1,method = "complete")
plot(hc_c2,cex=0.5,mian="")
```
#average-linkage.
```{r}
hc_c3=hclust(d1,method = "average")
plot(hc_c3,cex=0.5,mian="")
```
#ecludian distance
```{r}
d2=dist(g)
hc_c2=hclust(d2,method = "single")
plot(hc_c2,cex=0.5,mian="")
```
2)K-means algorithm

```{r}
k=10
SS_W=numeric(k)
km_fit=list()
for(i in 1:k){
  km_fit[[i]]=kmeans(g,i,iter.max = 50,nstart = 20)
  SS_W[i]=km_fit[[i]]$tot.withinss
}
  
```

```{r}
plot(1:k, SS_W,type="l",xlab="K",ylab="SS_W")
```

```{r}
plot(km_fit[[4]]$cluster)
df=data.frame(km_fit[[4]]$cluster)
df
```
#Linear Regression
```{r}
library(nclSLR)
data(diabetes)
dim(diabetes)
```
#split the data into train and test data
```{r}
set.seed(12)
train_data=data.frame(scale(diabetes[1:350,]))
test_data=data.frame(scale(diabetes[351:442,]))
Mean=colMeans(train_data)
Mean
standard_dev=apply(train_data,2,sd)
standard_dev

```
#fitting the multiple linear regression model
```{r}
model1=lm(train_data$dis~.,data=train_data)
summary(model1)
#fitting training model on test set
pred=predict(model1,newdata = test_data)
pred
#calculating MSE
MSE=mean((test_data$dis-pred)^2)
MSE
```
#apply the best subset selection algorithm on train_data
```{r}
library(leaps)
bss_fit=regsubsets(train_data$dis~.,data=train_data,method="exhaustive",nvmax=10)
bss_summary=summary(bss_fit)
bss_summary
str(bss_summary)
coef(bss_fit,6)

```   
```{r}
best_R=which.max(bss_summary$adjr2)
best_R
```
```{r}
best_cp=which.min(bss_summary$cp)
best_cp
```
```{r}
best_bic=which.min(bss_summary$bic)
best_bic
```
```{r}
train_data1=data.frame(train_data$sex,train_data$bmi,train_data$map,train_data$tc,train_data$hdl,train_data$ltg,train_data$dis)
colnames(train_data1)=c("sex","bmi","map","tc","hdl","ltg","dis")
test_data1=data.frame(test_data$sex,test_data$bmi,test_data$map,test_data$tc,test_data$hdl,test_data$ltg,test_data$dis)
colnames(test_data1)=c("sex","bmi","map","tc","hdl","ltg","dis")
model2=lm(dis~.,data=train_data1)
pred1=predict(model2,newdata = test_data1)
pred1
MSE2=mean((test_data1$dis-pred1)^2)
MSE2
```
#Rigid regression
```{r}
install.packages("glmnet",dependencies = TRUE)
library(glmnet)
```
```{r}
grid=10^seq(5,-3,length=100)
head(train_data)
X=train_data[,c(1:10)]
X1=as.matrix(X)
y=as.matrix(train_data$dis)
ridge_fit=glmnet(X1,y,aplha=0,standardize = FALSE,lambda = grid)
plot(ridge_fit,xvar="lambda",col=1:10,label = TRUE)
length(grid)
bet=coef(ridge_fit)
dim(bet)
grid[1]
bet[,1]
ridge_cv_fit=cv.glmnet(X1,y,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit)
```
#optimal value of the tuning parameter
```{r}
lambda_min=ridge_cv_fit$lambda.min
lambda_min
```
```{r}
i=which(ridge_cv_fit$lambda==ridge_cv_fit$lambda.min)
i
ridge_cv_fit$cvm[i]

```

