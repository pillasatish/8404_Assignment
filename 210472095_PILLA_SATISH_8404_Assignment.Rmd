---
title: "Assignmeant_8404"
author: "SATISH"
date: "11/4/2021"
output: html_document
---
1  Cluster Analysis
#Reading the dataset
```{r}
filename="C:/Users/pilla/Desktop/8404_Assignment/Ch10Ex11.csv"
gexpr=read.csv(filename,header = FALSE)
df=gexpr
dim(df)
```
#Transpose the matrix 
```{r}
g=t(gexpr)
dim(g)
```
#(a)Hierarchical clustering(correlation-based distance)
```{r}
dim(df)
r_obs=cor(df)
d_cor=1-r_obs
d1=as.dist(d_cor)
```
#single linkage
```{r}
hc_c=hclust(d1,method = "single")
plot(hc_c,cex=0.5,mian="")
```
#Examine the heights at which consecutive fusion occurs by plotting the height component of hc_c:
#Using cutree function to estimate the number of clusters when the dendrogram is cut at height of 0.9
```{r}
#hc_c$height
plot(hc_c$height,xlab = "fusion number",ylab = "fusion height")
hc_c_cut=cutree(hc_c,h=0.9)
hc_c_cut
```
#complete-linkage
```{r}
hc_c2=hclust(d1,method = "complete")
plot(hc_c2,cex=0.5,mian="")
```
#Examine the heights at which consecutive fusion occurs by plotting the height component of hc_c2:
#Using cutree function to estimate the number of clusters when the dendrogram is cut at height of 0.9
```{r}
plot(hc_c2$height,xlab = "fusion number",ylab = "fusion height")
hc_c_cut2=cutree(hc_c2,h=0.9)
hc_c_cut2
```
#average-linkage.
```{r}
hc_c3=hclust(d1,method = "average")
plot(hc_c3,cex=0.5,mian="")
```
#Examine the heights at which consecutive fusion occurs by plotting the height component of hc_c3:
#Using cutree function to estimate the number of clusters when the dendrogram is cut at height of 0.9
```{r}
plot(hc_c3$height,xlab = "fusion number",ylab = "fusion height")
hc_c_cut3=cutree(hc_c3,h=0.9)
hc_c_cut3
```
#ecludian distance
```{r}
d2=dist(g)
dim(g)
hc_c4=hclust(d2,method = "single")
plot(hc_c4,cex=0.5,mian="")

```
#Examine the heights at which consecutive fusion occurs by plotting the height component of hc_c4:
#Using cutree function to estimate the number of clusters when the dendrogram is cut at height of 45.5
```{r}
plot(hc_c4$height,xlab = "fusion number",ylab = "fusion height")
hc_c_cut4=cutree(hc_c4,h=45.5)
hc_c_cut4
```

#2)K-means algorithm

```{r}
set.seed(1)
k=10
SS_W=numeric(k)
SS_B=numeric(k)
km_fit=list()
for(i in 1:k){
  km_fit[[i]]=kmeans(g,i,iter.max = 50,nstart = 20)
  SS_W[i]=km_fit[[i]]$tot.withinss
  SS_B[i]=km_fit[[i]]$betweenss
}
km_fit[[2]]$cluster

```

```{r}
plot(1:k, SS_W,type="b",xlab="K",ylab="SS_W")
```
# distance between-cluster sum of squares(SSb)
```{r}
round(SS_B)
round(SS_W)
```

```{r}
pca_g=prcomp(x=g)
dim(g)
pca_g
km_fit[[4]]$cluster
plot(pca_g$x[,1],pca_g$x[,2],xlab="First PC",ylab="Second PC",col=km_fit[[4]]$cluster,pch=km_fit[[4]]$cluster)
```
#Linear Regression
```{r}
library(nclSLR)
data(diabetes)
dim(diabetes)
head(diabetes)
```
#split the data into train and test data
```{r}
train_data=data.frame(diabetes[1:350,])
test_data=data.frame(diabetes[351:442,])
```
#fitting the multiple linear regression model
```{r}
model1=lm(train_data$dis~.,data=train_data)
#summary(model1)
#fitting training model on test set
pred=predict(model1,newdata = test_data)
#pred
#calculating MSE
MSE=mean((test_data$dis-pred)^2)
MSE
```
#apply the best subset selection algorithm on train_data
```{r}
train_data1=data.frame(train_data$sex,train_data$bmi,train_data$map,train_data$tc,train_data$ldl,train_data$ltg,train_data$dis)
colnames(train_data1)=c("sex","bmi","map","tc","ldl","ltg","dis")
test_data1=data.frame(test_data$sex,test_data$bmi,test_data$map,test_data$tc,test_data$ldl,test_data$ltg,test_data$dis)
colnames(test_data1)=c("sex","bmi","map","tc","ldl","ltg","dis")
model2=lm(dis~.,data=train_data1)
pred1=predict(model2,newdata = test_data1)
#pred1
MSE2=mean((test_data1$dis-pred1)^2)
MSE2
```
#Rigid regression on train and test dataset
```{r}
install.packages("glmnet",dependencies = TRUE)
library(glmnet)
set.seed(1000)
grid=10^seq(5,-3,length=100)
head(train_data)
X=train_data[,c(1:10)]
X1=as.matrix(X)
y=train_data$dis
ridge_cv_fit=cv.glmnet(X1,y,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit)

```
#optimal value of the tuning parameter
```{r}
lambda_min=ridge_cv_fit$lambda.min
lambda_min
i=which(ridge_cv_fit$lambda==ridge_cv_fit$lambda.min)
i

```
#fixing the optimal value and fiting the model to all the training data and computing the test error over the validation data
```{r}
X2=test_data[,1:10]
#head(X2)
ridge_fit3=glmnet(X1,y,alpha = 0,standardize = FALSE,lambda=lambda_min)
pred3=predict(ridge_fit3,newx = as.matrix(X2),s=lambda_min)
MSE3=mean((test_data$dis-pred3)^2)
MSE3
```

#Based on full dataset
```{r}
set.seed(1)
A=as.matrix(diabetes[,1:10])
#A
b=diabetes$dis
#b
ridge_cv_fit1=cv.glmnet(A,b,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit1,xvar = "lambda",col=1:10,label = TRUE)
```
#optimal value of the tuning parameter based on full dataset
```{r}
lambda_min1=ridge_cv_fit1$lambda.min
lambda_min1
i1=which(ridge_cv_fit1$lambda==ridge_cv_fit1$lambda.min)
i1
```

#Plot showing how the estimates of the regression coefficients change as the tuning parameter is increased
```{r}
ridge_cv_fit2=glmnet(A,b,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit2,xvar = "lambda",col=1:10,label = TRUE)
```
#regression coefficients
```{r}

coef(ridge_cv_fit1,s=lambda_min1)

```
#regression coefficients are age,sex,tc,hdl are tending towards zero
#comapring with least least square method
```{r}
lsq_fit=lm(dis~.,data=diabetes)
coef(lsq_fit)
summary(lsq_fit)
```
#table to represent different test error for different methods
```{r}
df_table=data.frame(methods=c("least square Method","least square on 6 best variables","Ridge Method"),
                    Test_errors=c(MSE,MSE2,MSE3))
df_table
```

