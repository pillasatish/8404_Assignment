---
title: "Assignmeant_8404"
author: "SATISH"
date: "11/4/2021"
output: html_document
---
1  Cluster Analysis
#Reading the dataset
```{r}
filename="C:/Users/pilla/Desktop/8404_Assignment/Ch10Ex11.csv"
gexpr=read.csv(filename,header = FALSE)
df=gexpr
dim(df)
```
#Transpose the matrix 
```{r}
g=t(gexpr)
dim(g)
```
#(a)Hierarchical clustering(correlation-based distance)
```{r}
dim(df)
r_obs=cor(df)
d_cor=1-r_obs
d1=as.dist(d_cor)
```

```{r}
hc_c=hclust(d1,method = "single")
#class(hc_c)
#str(hc_c)
```
```{r}
plot(hc_c,cex=0.5,mian="")
```
```{r}
hc_c$height
plot(hc_c4$height)
cutree(hc_c4,h=46)
```

#complete-linkage
```{r}
hc_c2=hclust(d1,method = "complete")
plot(hc_c2,cex=0.5,mian="")
```
#average-linkage.
```{r}
hc_c3=hclust(d1,method = "average")
plot(hc_c3,cex=0.5,mian="")
```
#ecludian distance
```{r}
d2=dist(g)
dim(g)
hc_c4=hclust(d2,method = "single")
plot(hc_c4,cex=0.5,mian="")

```
2)K-means algorithm

```{r}
set.seed(1)
k=10
SS_W=numeric(k)
SS_W
SS_B=numeric(k)
round(SS_B)
km_fit=list()
for(i in 1:k){
  km_fit[[i]]=kmeans(g,i,iter.max = 50,nstart = 20)
  SS_W[i]=km_fit[[i]]$tot.withinss
  SS_B[i]=km_fit[[i]]$betweenss
}
km_fit[[5]]$cluster

```

```{r}
plot(1:k, SS_W,type="b",xlab="K",ylab="SS_W")
```
#between-cluster sum of squares(SSb)
```{r}
round(SS_B)
```

```{r}
pca_g=prcomp(x=g)
dim(g)
pca_g
km_fit[[4]]$cluster
plot(pca_g$x[,1],pca_g$x[,2],xlab="First PC",ylab="Second PC",col=km_fit[[4]]$cluster,pch=km_fit[[4]]$cluster)
```
#Linear Regression
```{r}
library(nclSLR)
data(diabetes)
dim(diabetes)
head(diabetes)
```
#split the data into train and test data
```{r}
set.seed(1)
train_data=data.frame(diabetes[1:350,])
test_data=data.frame(diabetes[351:442,])
```
#fitting the multiple linear regression model
```{r}
model1=lm(train_data$dis~.,data=train_data)
#summary(model1)
#fitting training model on test set
pred=predict(model1,newdata = test_data)
#pred
#calculating MSE
MSE=mean((test_data$dis-pred)^2)
MSE
```
#apply the best subset selection algorithm on train_data
```{r}
train_data1=data.frame(train_data$sex,train_data$bmi,train_data$map,train_data$tc,train_data$ldl,train_data$ltg,train_data$dis)
colnames(train_data1)=c("sex","bmi","map","tc","ldl","ltg","dis")
test_data1=data.frame(test_data$sex,test_data$bmi,test_data$map,test_data$tc,test_data$ldl,test_data$ltg,test_data$dis)
colnames(test_data1)=c("sex","bmi","map","tc","ldl","ltg","dis")
model2=lm(dis~.,data=train_data1)
pred1=predict(model2,newdata = test_data1)
#pred1
MSE2=mean((test_data1$dis-pred1)^2)
MSE2
```
#Rigid regression
```{r}
install.packages("glmnet",dependencies = TRUE)
library(glmnet)

grid=10^seq(5,-3,length=100)
head(train_data)
X=train_data[,c(1:10)]
X1=as.matrix(X)
y=train_data$dis
ridge_cv_fit=cv.glmnet(X1,y,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit)

```
#optimal value of the tuning parameter
```{r}
lambda_min=ridge_cv_fit$lambda.min
lambda_min
i=which(ridge_cv_fit$lambda==ridge_cv_fit$lambda.min)
i
X2=test_data[,1:10]
#X2
pred3=predict(ridge_cv_fit,newx = as.matrix(X2),s=lambda_min)
#pred3
MSE3=mean((test_data$dis-pred3)^2)
MSE3
```
#Based on full dataset
```{r}
A=as.matrix(diabetes[,1:10])
#A
b=diabetes$dis
#b
ridge_cv_fit1=glmnet(A,b,alpha=0,standardize=FALSE,lambda = grid)
plot(ridge_cv_fit1,xvar = "lambda",col=1:10,label = TRUE)
```
#regression coefficients
```{r}

coef(ridge_cv_fit1,s=lambda_min)
```
#regression coefficients are age,sex,tc,hdl are tending towards zero
#comapring with least least square method
```{r}
lsq_fit=lm(dis~.,data=diabetes)
coef(lsq_fit)
summary(lsq_fit)
```
